---
---



@inproceedings{10.1145/3706598.3713356,
author = {Yin, Yiwen and Mei, Yu and Yu, Chun and Li, Toby Jia-Jun and Jadoon, Aamir Khan and Cheng, Sixiang and Shi, Weinan and Chen, Mohan and Shi, Yuanchun},
title = {From Operation to Cognition: Automatic Modeling Cognitive Dependencies from User Demonstrations for GUI Task Automation},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713356},
doi = {10.1145/3706598.3713356},
abstract = {Traditional Programming by Demonstration (PBD) systems primarily automate tasks by recording and replaying operations on Graphical User Interfaces (GUIs), without fully considering the cognitive processes behind operations. This limits their ability to generalize tasks with interdependent operations to new contexts (e.g. collecting and summarizing introductions depending on different search keywords from varied websites). We propose TaskMind, a system that automatically identifies the semantics of operations, and the cognitive dependencies between operations from demonstrations, building a user-interpretable task graph. Users modify this graph to define new task goals, and TaskMind executes the graph to dynamically generalize new parameters for operations, with the integration of Large Language Models (LLMs). We compared TaskMind with a baseline end-to-end LLM which automates tasks from demonstrations and natural language commands, without task graph. In studies with 20 participants on both predefined and customized tasks, TaskMind significantly outperforms the baseline in both success rate and controllability.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {684},
numpages = {24},
keywords = {Programming by Demonstration, Task Automation, Task Graph, Cognitive Dependency},
location = {
},
series = {CHI '25},
selected = {true}
}

@article{10.1145/3397308,
author = {Liu, Guanhong and Gu, Yizheng and Yin, Yiwen and Yu, Chun and Wang, Yuntao and Mi, Haipeng and Shi, Yuanchun},
title = {Keep the Phone in Your Pocket: Enabling Smartphone Operation with an IMU Ring for Visually Impaired People},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
url = {https://doi.org/10.1145/3397308},
doi = {10.1145/3397308},
abstract = {Previous studies have shown that visually impaired users face a unique set of pain points in smartphone interaction including locating and removing the phone from a pocket, two-handed interaction while holding a cane, and keeping personal data private in a public setting. In this paper, we present a ring-based input interaction that enables in-pocket smartphone operation. By wearing a ring with an Inertial Measurement Unit on the index finger, users can perform gestures on any surface (e.g., tables, thighs) using subtle, one-handed gestures and receive auditory feedback via earphones. We conducted participatory studies to obtain a set of versatile commands and corresponding gestures. We subsequently trained an SVM model to recognize these gestures and achieved a mean accuracy of 95.5\% on 15 classifications. Evaluation results showed that our ring interaction is more efficient than some baseline phone interactions and is easy, private, and fun to use.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = jun,
articleno = {58},
numpages = {23},
keywords = {gestural input, accessibility},
selected = {true}
}
